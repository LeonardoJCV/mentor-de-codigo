# Mentor de C√≥digo Pessoal - Chat com IA 100% Local üöÄ

Este projeto apresenta o **Mentor de C√≥digo Pessoal**, uma aplica√ß√£o web que permite a desenvolvedores fazerem o upload de um projeto de c√≥digo-fonte e "conversarem" com ele atrav√©s de uma interface de chat, recebendo respostas geradas por um Large Language Model (LLM) que roda 100% offline na sua m√°quina.

‚ö†Ô∏è **Disclaimer:** Este √© um projeto de portf√≥lio para demonstrar habilidades em desenvolvimento full-stack e, principalmente, na implementa√ß√£o de uma arquitetura RAG (Retrieval-Augmented Generation) de ponta a ponta com ferramentas open-source.

---

## ‚úÖ Funcionalidades

-   üß† **Arquitetura RAG Local:** Implementa√ß√£o completa de Retrieval-Augmented Generation para fornecer respostas contextuais e precisas sobre o c√≥digo.
-   üìÇ **Upload Din√¢mico de Projetos:** Interface para fazer o upload de m√∫ltiplos arquivos de c√≥digo-fonte, que s√£o processados e vetorizados em tempo real.
-   ü§ñ **Chat Interativo com IA:** Converse com sua base de c√≥digo para entender funcionalidades, explicar componentes ou depurar l√≥gicas complexas.
-   üîí **100% Privado e Offline:** Todo o processamento, desde a vetoriza√ß√£o at√© a gera√ß√£o de texto pelo LLM, acontece localmente. Nenhum dado ou c√≥digo √© enviado para APIs externas.
-   ‚ö° **Stack Moderna Full-Stack:** Constru√≠do com as tecnologias mais atuais do ecossistema Python/FastAPI e React/TypeScript.

---

## üöÄ Tecnologias Utilizadas

Este projeto √© um **monorepo** com frontend e backend desacoplados.

#### **Frontend**

-   **Framework:** React com TypeScript
-   **Build Tool:** Vite
-   **Estiliza√ß√£o:** CSS puro
-   **Requisi√ß√µes HTTP:** Axios
-   **Roteamento:** React Router
-   **Empacotamento de Arquivos:** JSZip

#### **Backend & IA**

-   **Framework:** Python com FastAPI
-   **Servidor:** Uvicorn
-   **Orquestra√ß√£o de IA:** LangChain
-   **LLM Local:** Ollama (executando Llama 3)
-   **Banco de Dados Vetorial:** ChromaDB
-   **Modelo de Embedding:** Hugging Face `all-MiniLM-L6-v2`
-   **Valida√ß√£o de Dados:** Pydantic

---

## ‚öôÔ∏è Como Executar

### Pr√©-requisitos

Antes de come√ßar, garanta que voc√™ possui:

-   [Node.js e npm](https://nodejs.org/en/) (v18+)
-   [Python](https://www.python.org/downloads/) (v3.8+)
-   [Ollama](https://ollama.com/)
    -   Ap√≥s instalar o Ollama, baixe o Llama 3 (isso pode levar um tempo):
      ```bash
      ollama pull llama3
      ```

### Instala√ß√£o e Execu√ß√£o

1.  Clone este reposit√≥rio:
    ```bash
    git clone https://github.com/SEU_USUARIO/SEU_REPOSITORIO.git
    cd SEU_REPOSITORIO
    ```

2.  **Inicie o Ollama** (em um terminal dedicado):
    ```bash
    ollama serve
    ```

3.  **Configure e execute o Backend** (em um segundo terminal):
    ```bash
    cd backend
    python -m venv venv
    # Ative o ambiente virtual (.\venv\Scripts\activate no Windows)
    source venv/bin/activate
    pip install -r requirements.txt
    uvicorn main:app --reload
    ```

4.  **Configure e execute o Frontend** (em um terceiro terminal):
    ```bash
    cd frontend
    npm install
    npm run dev
    ```

5.  Acesse a aplica√ß√£o no seu navegador: **[http://localhost:5173](http://localhost:5173)**

---

## üé® Vis√£o do Projeto

Criado como um estudo aprofundado para dominar a constru√ß√£o de sistemas de IA locais e privados. Este projeto serve como uma prova de conceito.

¬© 2025 - LeonardoJCV
